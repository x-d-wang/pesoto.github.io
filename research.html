---
layout: page
title: Research
permalink: /research/
---

<strong><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8672861">Fast Adaptive K-Means Subspace Clustering for High-Dimensional Data</a></strong><br><br>
<p style="text-align:justify"><i>Abstract: </i>In many real-world applications, data are represented by high-dimensional features. Despite the simplicity, existing K-means subspace clustering algorithms often employ eigenvalue decomposition to generate an approximate solution, which makes the model less efficiency. Besides, their loss functions are either sensitive to outliers or small loss errors. In this paper, we propose a fast adaptive K-means (FAKM) type subspace clustering model, where an adaptive loss function is designed to provide a fexible cluster indicator calculation mechanism, thereby suitable for datasets under different distributions. To find the optimal feature subset, FAKM performs clustering and feature selection simultaneously without the eigenvalue decomposition, therefore efficient for real-world applications.We exploit an efficient alternative optimization algorithm to solve the proposed model, together with theoretical analyses on its convergence and computational complexity. Finally, extensive experiments on several benchmark datasets demonstrate the advantages of FAKM compared to state-of-the-art clustering algorithms.</p>
<br>
<strong><a href="https://www.sciencedirect.com/science/article/abs/pii/S0262885617300859">Semi-supervised multi-label feature selection via label correlation analysis with l1-norm graph embedding</a></strong><br><br>
<p style="text-align:justify"><i>Abstract: </i>In this paper, we propose a novel semi-supervised multi-label feature selection algorithm and apply it to three different applications: natural scene classification, web page annotation, and yeast gene functional classification. Compared with the previous works, there are two advantages of our algorithm: (1) Manifold learning which leverages the underlying geometric structure of the training data is imposed to utilize both labeled and unlabeled data. Besides, the underlyingmanifold structure is guaranteed to be clear by using the l1-norm regularization. (2) Shared subspace learning which has shown its efficiency in multi-label learning scenarios, is also considered in our feature learning algorithm. The proposed objective function involves l2,1-norm and l1-norm, making it non-smooth and difficult to solve. We also design an efficient iterative algorithm to optimize it. Experimental results demonstrate the effectiveness of our algorithm compared with sate-of-the-art algorithms on different tasks.</p><br>
<br>
<strong><a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231216300376"> Unsupervised spectral feature selection with l1-norm graph</a></strong><br>with Andrea Fabiani, Martha L&oacute;pez Pi&ntilde;eros and Jos&eacute;-Luis Peydr&oacute;<br><br>
<p style="text-align:justify"><i>Abstract: </i>Feature selection, which aims to reduce redundancy or noise in the original feature sets, plays an important role in many applications, such as machine learning, multimedia analysis and data mining. Spectral feature selection, a recently proposed method, makes use of spectral clustering to capture underlying manifold structure and achieves excellent performance. However, existing Spectral feature selections suffer from imposing kinds of constraints and lack of clear manifold structure. To address this problem, we propose a new Unsupervised Spectral Feature Selection with l1-Norm Graph, namely USFS. Different from most state-of-art algorithms, the proposed algorithm performs the spectral clustering and l1-Norm Graph jointly to select discriminative features. The manifold structure of original datasets is first learned by the spectral clustering from unlabeled samples, and then it is used to guide the feature selection procedure. Moreover, l1-Norm Graph is imposed to capture clear manifold structure. We also present an efficient iterative optimize method and theoretical convergence analysis of the proposed algorithm. Extensive experimental results on real-world datasets demonstrate the performance of the proposed algorithm.</p><br>
